{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"05dce7a6de0b41c199a01220413e8dd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86a55e98dee4458c99e28945106c84ca","IPY_MODEL_97c5ee56e38f40aa9ddc8db0e10bf82b","IPY_MODEL_e4f8befe9d3a4b4e87ee31b2e0b55c08"],"layout":"IPY_MODEL_2942a83bf195404e8ee15f2a843e1fdb"}},"86a55e98dee4458c99e28945106c84ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e469257ea9f48c3aa7134f2589827a9","placeholder":"​","style":"IPY_MODEL_9f34299b502d4d238b2b5d3f79010161","value":"Sanity Checking DataLoader 0: 100%"}},"97c5ee56e38f40aa9ddc8db0e10bf82b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5502501ba1f04d0990a7bba9933b561c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ead18e5ac6e459c9b48708f0881fc43","value":2}},"e4f8befe9d3a4b4e87ee31b2e0b55c08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5940be2bbb92418f89c6e74f9c55c2e7","placeholder":"​","style":"IPY_MODEL_09e587b784354eceb9182df05b16c2ed","value":" 2/2 [01:07&lt;00:00, 33.82s/it]"}},"2942a83bf195404e8ee15f2a843e1fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"0e469257ea9f48c3aa7134f2589827a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f34299b502d4d238b2b5d3f79010161":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5502501ba1f04d0990a7bba9933b561c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ead18e5ac6e459c9b48708f0881fc43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5940be2bbb92418f89c6e74f9c55c2e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09e587b784354eceb9182df05b16c2ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4778e12584e1472ca8040ffa64cbadf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8a95af5ea484fdda92dacb8000aadc1","IPY_MODEL_ba3715bdcfb9420092d0438dec79367d","IPY_MODEL_3496f8d7b338491bac9bb262f9b42215"],"layout":"IPY_MODEL_6c26df62a5404761ba67f0bb87337439"}},"d8a95af5ea484fdda92dacb8000aadc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a01a544f6ece4955a401e527777c7b7d","placeholder":"​","style":"IPY_MODEL_f923cc7f63c84c608cb5c111b922af09","value":"Epoch 0:   0%"}},"ba3715bdcfb9420092d0438dec79367d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f091d77987e4e7d9eb7d4cbb1d8dd66","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f65961b94384e0b9d9fbee0bc5c0fdc","value":0}},"3496f8d7b338491bac9bb262f9b42215":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d6a796aaf6749b998ed75ea07e07898","placeholder":"​","style":"IPY_MODEL_5543e90844b443fb82fb9ca3721c7280","value":" 0/10000 [00:00&lt;?, ?it/s]"}},"6c26df62a5404761ba67f0bb87337439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"a01a544f6ece4955a401e527777c7b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f923cc7f63c84c608cb5c111b922af09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f091d77987e4e7d9eb7d4cbb1d8dd66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f65961b94384e0b9d9fbee0bc5c0fdc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d6a796aaf6749b998ed75ea07e07898":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5543e90844b443fb82fb9ca3721c7280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import shutil\n# import os\n# import zipfile\n# # shutil.unpack_archive(\"gdrive/My Drive/train_dataset.tar.gz\", \"/content/gdrive/My Drive/FYP_dataset\")\n\n# os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"","metadata":{"id":"3sph2EUiJ-ih","execution":{"iopub.status.busy":"2022-10-17T16:35:49.755601Z","iopub.execute_input":"2022-10-17T16:35:49.756532Z","iopub.status.idle":"2022-10-17T16:35:49.760465Z","shell.execute_reply.started":"2022-10-17T16:35:49.756494Z","shell.execute_reply":"2022-10-17T16:35:49.759631Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !kaggle datasets download -d shahrukhkhan/im2latex100k","metadata":{"id":"KuR0A0lmawDB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c9f5be9-033a-4fc0-b0ba-9558a2c8f65b","execution":{"iopub.status.busy":"2022-10-17T16:36:23.789114Z","iopub.execute_input":"2022-10-17T16:36:23.789490Z","iopub.status.idle":"2022-10-17T16:36:23.794059Z","shell.execute_reply.started":"2022-10-17T16:36:23.789458Z","shell.execute_reply":"2022-10-17T16:36:23.792973Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# zip_ref = zipfile.ZipFile('/content/im2latex100k.zip', 'r') #Opens the zip file in read mode\n# zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n# zip_ref.close()","metadata":{"id":"T3t6kgBgWtdl","execution":{"iopub.status.busy":"2022-10-17T16:36:23.799572Z","iopub.execute_input":"2022-10-17T16:36:23.799864Z","iopub.status.idle":"2022-10-17T16:36:23.803992Z","shell.execute_reply.started":"2022-10-17T16:36:23.799837Z","shell.execute_reply":"2022-10-17T16:36:23.803020Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# len(os.listdir('/tmp/formula_images_processed/formula_images_processed/'))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSRRvmkpVT-r","outputId":"72275ed7-37ae-4e5a-f859-4fb2e831b6f4","execution":{"iopub.status.busy":"2022-10-17T16:36:24.699300Z","iopub.execute_input":"2022-10-17T16:36:24.699819Z","iopub.status.idle":"2022-10-17T16:36:24.705417Z","shell.execute_reply.started":"2022-10-17T16:36:24.699735Z","shell.execute_reply":"2022-10-17T16:36:24.704205Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install jiwer\n!pip install wandb\n!pip install distance\n# !pip install pytorch_lightning","metadata":{"id":"tB-22y9jD5GM","execution":{"iopub.status.busy":"2022-10-26T15:05:19.056675Z","iopub.execute_input":"2022-10-26T15:05:19.057090Z","iopub.status.idle":"2022-10-26T15:05:48.024958Z","shell.execute_reply.started":"2022-10-26T15:05:19.057049Z","shell.execute_reply":"2022-10-26T15:05:48.023800Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jiwer in /opt/conda/lib/python3.7/site-packages (2.5.1)\nRequirement already satisfied: levenshtein==0.20.2 in /opt/conda/lib/python3.7/site-packages (from jiwer) (0.20.2)\nRequirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from levenshtein==0.20.2->jiwer) (2.12.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.9.8)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.12.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.3.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: distance in /opt/conda/lib/python3.7/site-packages (0.1.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.checkpoint import checkpoint\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn, Tensor\n# import torchdata.datapipes as dp\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport torchvision\nfrom torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\nimport glob\nfrom tqdm.notebook import tqdm\nimport time\nfrom jiwer import wer as cal_wer\n# from nltk.metrics import edit_distance\nfrom typing import Tuple\nimport pytorch_lightning as pl\nimport json\nimport random","metadata":{"id":"6vzyPHlDD5GH","execution":{"iopub.status.busy":"2022-10-26T15:06:12.380154Z","iopub.execute_input":"2022-10-26T15:06:12.380897Z","iopub.status.idle":"2022-10-26T15:06:16.905349Z","shell.execute_reply.started":"2022-10-26T15:06:12.380842Z","shell.execute_reply":"2022-10-26T15:06:16.904142Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"bs = 2\nepochs = 20\nmax_length = 150\nlog_idx = 300\nworkers = 2\n\ncuda = torch.cuda.is_available()  \ndevice = torch.device('cuda' if cuda else 'cpu')\n# device = 'cpu'\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ct71fSRoD5GP","outputId":"03fd3bdf-003a-40cc-864b-f020ef70dbd3","execution":{"iopub.status.busy":"2022-10-26T15:06:19.338010Z","iopub.execute_input":"2022-10-26T15:06:19.338373Z","iopub.status.idle":"2022-10-26T15:06:19.346025Z","shell.execute_reply.started":"2022-10-26T15:06:19.338341Z","shell.execute_reply":"2022-10-26T15:06:19.345047Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"import re\nclass Text:\n    def __init__(self):\n        self.tokens = json.load(open(\"../input/latex-token/latex_tokens.json\", \"r\"))\n        self.map_tokens = dict(zip(self.tokens, range(len(self.tokens))))\n        self.pad_id = 0\n        self.sos_id = 1\n        self.eos_id = 2\n        self.TOKENIZE_PATTERN = re.compile(\n            \"(\\\\\\\\[a-zA-Z]+)|\"\n            +  # \\[command name]\n            # \"(\\{\\w+?\\})|\"+ # {[text-here]} Check if this is needed\n            '((\\\\\\\\)*[$-/:-?{-~!\"^_`\\[\\]])|'\n            + \"(\\w)|\"  # math symbols\n            + \"(\\\\\\\\)\"  # single letters or other chars\n        )  # \\ characters\n\n    def tokenize(self, formula: str):\n        \"\"\"Returns list of tokens in given formula.\n        formula - string containing the LaTeX formula to be tokenized\n        Note: Somewhat work-in-progress\"\"\"\n        tokens = re.finditer(self.TOKENIZE_PATTERN, str(formula))\n        tokens = list(map(lambda x: x.group(0), tokens))\n        tokens = [x for x in tokens if x is not None and x != \"\"]\n        return tokens\n\n    def int2text(self, x: int):\n        if  x > self.eos_id :\n            \n             return self.tokens[x] \n\n    def int2text_n(self, x: Tensor):\n        return \"\".join([self.tokens[i] for i in x if i > self.eos_id])\n\n    def text2int(self, formula: str):\n        return torch.LongTensor([self.map_tokens[i] for i in self.tokenize(formula)])","metadata":{"id":"ES_CJRhaD5GT","execution":{"iopub.status.busy":"2022-10-26T15:13:01.440974Z","iopub.execute_input":"2022-10-26T15:13:01.441333Z","iopub.status.idle":"2022-10-26T15:13:01.451867Z","shell.execute_reply.started":"2022-10-26T15:13:01.441302Z","shell.execute_reply":"2022-10-26T15:13:01.450772Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"text = Text()\nn_class = len(text.tokens)\nn_class\n520","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyWF-lULD5GU","outputId":"5832efd4-9ecd-4be4-9bbd-d972db66648c","execution":{"iopub.status.busy":"2022-10-26T15:13:05.365966Z","iopub.execute_input":"2022-10-26T15:13:05.366447Z","iopub.status.idle":"2022-10-26T15:13:05.375238Z","shell.execute_reply.started":"2022-10-26T15:13:05.366372Z","shell.execute_reply":"2022-10-26T15:13:05.374340Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"520"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\ndata_path = Path('../input/file-30')\nimg_path = Path('../input/im2latex100k/formula_images_processed/formula_images_processed')\nclass LatexDataset(Dataset):\n    def __init__(self, data_type: str):\n        super().__init__()\n        assert data_type in ['train', 'test', 'val'], 'Not found data type'\n        csv_path = data_path / f'df_{data_type}_new.csv'\n        df = pd.read_csv(csv_path)\n        df['image'] = df.image.map(lambda x: img_path / x)\n        self.walker = df.to_dict('records')\n        \n    def __len__(self):\n        return len(self.walker)\n    \n    def __getitem__(self, idx):\n        item = self.walker[idx]\n        \n        formula = item['formula']\n        image = torchvision.io.read_image(str(item['image']))\n        \n        return image, formula","metadata":{"id":"EeD0aizUD5GV","execution":{"iopub.status.busy":"2022-10-26T15:08:53.635241Z","iopub.execute_input":"2022-10-26T15:08:53.635618Z","iopub.status.idle":"2022-10-26T15:08:53.643468Z","shell.execute_reply.started":"2022-10-26T15:08:53.635586Z","shell.execute_reply":"2022-10-26T15:08:53.642448Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_set = LatexDataset('train')\nval_set = LatexDataset('val')\ntest_set = LatexDataset('test')\n\nlen(train_set), len(val_set), len(test_set)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2Zc33UnD5GW","outputId":"c986e5c7-aaac-430f-cdee-5dca58af5637","execution":{"iopub.status.busy":"2022-10-26T15:17:10.324841Z","iopub.execute_input":"2022-10-26T15:17:10.325201Z","iopub.status.idle":"2022-10-26T15:17:10.911490Z","shell.execute_reply.started":"2022-10-26T15:17:10.325168Z","shell.execute_reply":"2022-10-26T15:17:10.910383Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(30000, 3000, 5000)"},"metadata":{}}]},{"cell_type":"code","source":"def collate_fn(batch):\n    formulas = [text.text2int(i[1]) for i in batch]\n    formulas = pad_sequence(formulas, batch_first=True)\n    sos = torch.zeros(bs, 1) + text.map_tokens['<s>']\n    eos = torch.zeros(bs, 1) + text.map_tokens['<e>']\n    try:\n        formulas = torch.cat((sos, formulas, eos), dim=-1).to(dtype=torch.long)\n    except Exception:\n        pass\n        \n    image = [i[0] for i in batch]\n    width, height = 160, 200\n    for img in image:\n        c, h, w = img.size()\n        # max_width = max(max_width, w)\n        # max_height = max(max_height, h)\n    pad = torchvision.transforms.Resize(size=(height, width))\n    image = torch.stack(list(map(lambda x: pad(x), image))).to(dtype=torch.float)\n    return image, formulas\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, train_set, val_set, test_set):\n        super().__init__()\n        self.train_set = train_set\n        self.val_set = val_set\n        self.test_set = test_set\n\n    def train_dataloader(self):\n        return DataLoader(self.train_set,\n                          shuffle=True, \n                          batch_size=bs,\n                          num_workers=workers,\n                          collate_fn=collate_fn)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_set,\n                          shuffle=False, \n                          batch_size=bs,\n                          num_workers=workers,\n                          collate_fn=collate_fn)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_set,\n                          shuffle=False, \n                          batch_size=bs,\n                          num_workers=workers,\n                          collate_fn=collate_fn)","metadata":{"id":"3rIzONt_D5GX","execution":{"iopub.status.busy":"2022-10-26T15:18:36.185462Z","iopub.execute_input":"2022-10-26T15:18:36.185812Z","iopub.status.idle":"2022-10-26T15:18:36.198484Z","shell.execute_reply.started":"2022-10-26T15:18:36.185781Z","shell.execute_reply":"2022-10-26T15:18:36.197552Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"dm = DataModule(train_set, val_set, test_set)","metadata":{"id":"PNzNORjrD5GZ","execution":{"iopub.status.busy":"2022-10-26T15:18:38.901907Z","iopub.execute_input":"2022-10-26T15:18:38.902281Z","iopub.status.idle":"2022-10-26T15:18:38.907130Z","shell.execute_reply.started":"2022-10-26T15:18:38.902248Z","shell.execute_reply":"2022-10-26T15:18:38.905783Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\nimport math\n\ndef add_positional_features(tensor: torch.Tensor,\n                            min_timescale: float = 1.0,\n                            max_timescale: float = 1.0e4):\n    \"\"\"\n    Implements the frequency-based positional encoding described\n    in `Attention is all you Need\n    Parameters\n    ----------\n    tensor : ``torch.Tensor``\n        a Tensor with shape (batch_size, timesteps, hidden_dim).\n    min_timescale : ``float``, optional (default = 1.0)\n        The largest timescale to use.\n    Returns\n    -------\n    The input tensor augmented with the sinusoidal frequencies.\n    \"\"\"\n    _, timesteps, hidden_dim = tensor.size()\n\n    timestep_range = get_range_vector(timesteps, tensor.device).data.float()\n    # We're generating both cos and sin frequencies,\n    # so half for each.\n    num_timescales = hidden_dim // 2\n    timescale_range = get_range_vector(\n        num_timescales, tensor.device).data.float()\n\n    log_timescale_increments = math.log(\n        float(max_timescale) / float(min_timescale)) / float(num_timescales - 1)\n    inverse_timescales = min_timescale * \\\n        torch.exp(timescale_range * -log_timescale_increments)\n\n    # Broadcasted multiplication - shape (timesteps, num_timescales)\n    scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)\n    # shape (timesteps, 2 * num_timescales)\n    sinusoids = torch.randn(\n        scaled_time.size(0), 2*scaled_time.size(1), device=tensor.device)\n    sinusoids[:, ::2] = torch.sin(scaled_time)\n    sinusoids[:, 1::2] = torch.sin(scaled_time)\n    if hidden_dim % 2 != 0:\n        # if the number of dimensions is odd, the cos and sin\n        # timescales had size (hidden_dim - 1) / 2, so we need\n        # to add a row of zeros to make up the difference.\n        sinusoids = torch.cat(\n            [sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)\n    return tensor + sinusoids.unsqueeze(0)\n\n\ndef get_range_vector(size: int, device) -> torch.Tensor:\n    return torch.arange(0, size, dtype=torch.long, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:18:52.395085Z","iopub.execute_input":"2022-10-26T15:18:52.395453Z","iopub.status.idle":"2022-10-26T15:18:52.406116Z","shell.execute_reply.started":"2022-10-26T15:18:52.395414Z","shell.execute_reply":"2022-10-26T15:18:52.405030Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nclass Encoder(nn.Module):\n    def __init__(self, enc_out_dim: int, add_pos_feat = True):\n        super(Encoder, self).__init__()\n#         self.conv = nn.Sequential(\n#             nn.Conv2d(3, 64, 3, 1, 1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2, 1),\n#             nn.BatchNorm2d(64),\n            \n#             nn.Conv2d(64, 128, 3, 1, 1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2, 1),\n#             nn.BatchNorm2d(128),\n            \n#             nn.Conv2d(128, 256, 3, 1, 1),\n#             nn.ReLU(),\n#             nn.BatchNorm2d(256),\n            \n#             nn.Conv2d(256, 256, 3, 1, 1),\n#             nn.ReLU(),\n#             nn.BatchNorm2d(256),\n#             nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n#             nn.ReLU()\n#         )\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, 3, 1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 256, 3, 1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 256, 3, 1),\n            nn.ReLU()\n            nn.MaxPool2d(2, 1),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 512, 3, 1),\n            nn.ReLU()\n            nn.MaxPool2d(1, 2),\n#             nn.BatchNorm2d(512),\n            nn.Conv2d(512, enc_out_dim, 3, 1, 0),\n            nn.ReLU()\n\n        )\n        self.add_pos_feat = add_pos_feat\n    \n\n    def forward(self, X: Tensor):\n        \"\"\"\n            x: (bs, c, w, h)\n        \"\"\"\n        features = self.conv(X)\n        features = features.permute(0, 2, 3, 1)\n#         features = features.view(features.size(0), -1, features.size(-1)) \n        B, H, W, _ = features.shape\n        features = features.contiguous().view(B, H*W, -1)\n        \n        if self.add_pos_feat :\n            features = add_positional_features(features)\n        return features","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:18:56.267667Z","iopub.execute_input":"2022-10-26T15:18:56.268018Z","iopub.status.idle":"2022-10-26T15:18:56.280571Z","shell.execute_reply.started":"2022-10-26T15:18:56.267988Z","shell.execute_reply":"2022-10-26T15:18:56.279543Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.distributions.uniform import Uniform","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:19:02.647846Z","iopub.execute_input":"2022-10-26T15:19:02.648242Z","iopub.status.idle":"2022-10-26T15:19:02.655710Z","shell.execute_reply.started":"2022-10-26T15:19:02.648207Z","shell.execute_reply":"2022-10-26T15:19:02.653096Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"INIT = 1e-2\nclass Decoder(nn.Module):\n    def __init__(self, out_size, emb_size, dec_rnn_h, enc_out_dim, n_layer = 1, dropout = 0., beam_width = 5, text = text):\n        super(Decoder, self).__init__()\n        self.rnn_decoder = nn.LSTMCell(dec_rnn_h + emb_size, dec_rnn_h)\n        self.embedding = nn.Embedding(out_size, emb_size)\n        \n        self.init_wh = nn.Linear(enc_out_dim, dec_rnn_h)\n        self.init_wc = nn.Linear(enc_out_dim, dec_rnn_h)\n        self.init_wo = nn.Linear(enc_out_dim, dec_rnn_h)\n        \n        self.beta = nn.Parameter(torch.Tensor(enc_out_dim))\n        init.uniform_(self.beta, -INIT, INIT)\n        self.W_1 = nn.Linear(enc_out_dim, enc_out_dim, bias = False)\n        self.W_2 = nn.Linear(dec_rnn_h, enc_out_dim, bias = False)\n        \n        self.W_3 = nn.Linear(dec_rnn_h + enc_out_dim, dec_rnn_h, bias = False)\n        self.W_out = nn.Linear(dec_rnn_h, out_size, bias = False)\n        \n        self.dropout = nn.Dropout(p= dropout)\n        self.uniform  = Uniform(0, 1)\n        self.beam_width = beam_width\n        self.logsoftmax = nn.LogSoftmax(dim=-1)\n        self.text  = text\n        \n        \n    def forward(self, features, formulas, epsilon=1.):\n        \"\"\"args:\n        imgs: [B, C, H, W]\n        formulas: [B, MAX_LEN]\n        epsilon: probability of the current time step to\n                 use the true previous token\n        return:\n        logits: [B, MAX_LEN, VOCAB_SIZE]\n        \"\"\"\n        # encoding\n        # init decoder's states\n        dec_states, o_t = self.init_decoder(features)\n        max_len = formulas.size(1)\n        logits = []\n        for t in range(max_len):\n            tgt = formulas[:, t:t+1]\n            # schedule sampling\n#             if logits and self.uniform.sample().item() > epsilon:\n#                 tgt = torch.argmax(torch.log(logits[-1]), dim=1, keepdim=True)\n            # ont step decoding\n            dec_states, o_t, logit = self.decode_step(\n                dec_states, o_t, features, tgt)\n            logits.append(logit)\n        logits = torch.stack(logits, dim=1)  # [B, MAX_LEN, out_size]\n        return logits\n        \n        \n    def decode_step(self, dec_states, o_t, enc_out, tgt):\n        \"\"\"Runing one step decoding\"\"\"\n\n        prev_y = self.embedding(tgt).squeeze(1)  # [B, emb_size]\n        inp = torch.cat([prev_y, o_t], dim=1)  # [B, emb_size+dec_rnn_h]\n        h_t, c_t = self.rnn_decoder(inp, dec_states)  # h_t:[B, dec_rnn_h]\n        h_t = self.dropout(h_t)\n        c_t = self.dropout(c_t)\n\n        # context_t : [B, C]\n        context_t, attn_scores = self._get_attn(enc_out, h_t)\n\n        # [B, dec_rnn_h]\n        o_t = self.W_3(torch.cat([h_t, context_t], dim=1)).tanh()\n        o_t = self.dropout(o_t)\n\n        # calculate logit\n        logit = self.logsoftmax(self.W_out(o_t))  # [B, out_size]\n\n        return (h_t, c_t), o_t, logit\n    \n    def _get_attn(self, enc_out, h_t):\n        \"\"\"Attention mechanism\n        args:\n            enc_out: row encoder's output [B, L=H*W, C]\n            h_t: the current time step hidden state [B, dec_rnn_h]\n        return:\n            context: this time step context [B, C]\n            attn_scores: Attention scores\n        \"\"\"\n        # cal alpha\n        alpha = torch.tanh(self.W_1(enc_out)+self.W_2(h_t).unsqueeze(1))\n        alpha = torch.sum(self.beta*alpha, dim=-1)  # [B, L]\n        alpha = F.softmax(alpha, dim=-1)  # [B, L]\n\n        # cal context: [B, C]\n        context = torch.bmm(alpha.unsqueeze(1), enc_out)\n        context = context.squeeze(1)\n        return context, alpha\n                       \n    def generate_formulas(self, features, max_len, type):\n        assert type in ['greedy', 'beam']\n        if type == 'greedy':\n            dec_states, O_t = self.init_decoder(features)\n            batch_size = features.size(0)\n            tgt = torch.ones(batch_size, 1, device = device).long() * self.text.sos_id\n#             formulas_idx = torch.ones(batch_size, max_len, device=device).long() * self.text.pad_id\n            result = []\n            for t in range(max_len):\n                dec_states, O_t, logit = self.decode_step(dec_states, O_t, features, tgt)\n                \n                tgt = torch.argmax(logit, dim=1)\n                result.append(tgt.item())\n            return result\n        \n        if type == 'beam':\n            dec_states, O_t = self.init_decoder(features)\n            batch_size = features.size(0)\n            list_candidates = [([self.text.sos_id], dec_states, O_t, 0.0)]\n            for t in range(max_length):\n                new_candidates = []\n                for inp, dec_states, O_t, log_prob in list_candidates:\n                       tgt = torch.ones(batch_size, 1, device = device).long() * inp[-1]\n                       dec_states, O_t, logit = self.decode_step(dec_states, O_t, features, tgt)\n                       topk = logit.topk(self.beam_width)\n                       new_log_prob = topk.values.view(-1).tolist()\n                       new_idx = topk.indices.view(-1).tolist()\n                       for val, idx in zip(new_log_prob, new_idx):\n                            new_inp = inp + [idx]\n                            new_candidates.append((new_inp, dec_states, O_t, log_prob + val))\n                new_candidates = sorted(new_candidates, key = lambda x: x[3], reverse=True)\n                list_candidate = new_candidates[:self.beam_width]\n            return list_candidate[0][0]\n    \n\n    def _init_h(self, mean_enc_out):\n        return torch.tanh(self.init_wh(mean_enc_out))\n    \n    \n    \n    def _init_c(self, mean_enc_out):\n        return torch.tanh(self.init_wc(mean_enc_out))\n    \n    def _init_o(self, mean_enc_out):\n        return torch.tanh(self.init_wo(mean_enc_out))\n    \n    def init_decoder(self, enc_out):\n        '''\n        args:\n        enc_out : the output of row encoder [B, H*W, C]\n        return:\n        h_0, c_0 : shape [B, dec_rnn_h]\n        init_o : the average of enc_out [B, dec_rnn_h]\n        '''\n        mean_enc_out = enc_out.mean(dim=1)\n        h = self._init_h(mean_enc_out)\n        c = self._init_c(mean_enc_out)\n        init_o = self._init_o(mean_enc_out)\n        return (h, c), init_o\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:19:04.970777Z","iopub.execute_input":"2022-10-26T15:19:04.971120Z","iopub.status.idle":"2022-10-26T15:19:04.999417Z","shell.execute_reply.started":"2022-10-26T15:19:04.971089Z","shell.execute_reply":"2022-10-26T15:19:04.998382Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self, enc_out_dim, out_size, emb_size, dec_rnn_h, text):\n        super().__init__()\n        self.encoder = Encoder(enc_out_dim = enc_out_dim)\n        self.decoder = Decoder(out_size = out_size, dec_rnn_h = dec_rnn_h, emb_size = emb_size, enc_out_dim= enc_out_dim, text= text)\n    def forward(self, images,formulas):\n        features = self.encoder(images)\n        outputs = self.decoder(features, formulas)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:19:10.344043Z","iopub.execute_input":"2022-10-26T15:19:10.344388Z","iopub.status.idle":"2022-10-26T15:19:10.350664Z","shell.execute_reply.started":"2022-10-26T15:19:10.344356Z","shell.execute_reply":"2022-10-26T15:19:10.349634Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport distance\nimport numpy as np\n\ndef edit_distance(references, hypotheses):\n    \"\"\"Computes Levenshtein distance between two sequences.\n    Args:\n        references: list of list of token (one hypothesis)\n        hypotheses: list of list of token (one hypothesis)\n    Returns:\n        1 - levenshtein distance: (higher is better, 1 is perfect)\n    \"\"\"\n    d_leven, len_tot = 0, 0\n    for ref, hypo in zip(references, hypotheses):\n        d_leven += distance.levenshtein(ref, hypo)\n        len_tot += float(max(len(ref), len(hypo)))\n\n    return 1. - d_leven / len_tot\n\ndef calculate_bleu_score(references, hypotheses):\n    \"\"\"Computes bleu score.\n    Args:\n        references: list of list (one hypothesis)\n        hypotheses: list of list (one hypothesis)\n    Returns:\n        BLEU-4 score: (float)\n    \"\"\"\n    references = [[ref] for ref in references]  # for corpus_bleu func\n    BLEU_4 = nltk.translate.bleu_score.corpus_bleu(\n        references, hypotheses,\n        weights=(0.25, 0.25, 0.25, 0.25)\n    )\n    return BLEU_4\n\ndef exact_match_score(references, hypotheses):\n    \"\"\"Computes exact match scores.\n    Args:\n        references: list of list of tokens (one ref)\n        hypotheses: list of list of tokens (one hypothesis)\n    Returns:\n        exact_match: (float) 1 is perfect\n    \"\"\"\n    exact_match = 0\n    for ref, hypo in zip(references, hypotheses):\n        if np.array_equal(ref, hypo):\n            exact_match += 1\n\n    return exact_match / float(max(len(hypotheses), 1))","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:19:18.343859Z","iopub.execute_input":"2022-10-26T15:19:18.344222Z","iopub.status.idle":"2022-10-26T15:19:19.164824Z","shell.execute_reply.started":"2022-10-26T15:19:18.344174Z","shell.execute_reply":"2022-10-26T15:19:19.163844Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.metrics import bleu_score\nclass Image2LatexModel(pl.LightningModule):\n    def __init__(self, init_lr, max_lr, total_steps, **kwargs):\n        super().__init__()\n        self.model = EncoderDecoder(**kwargs)\n        self.criterion = nn.CrossEntropyLoss()\n        self.init_lr = init_lr\n        self.max_lr = max_lr\n        self.total_steps = total_steps\n        self.save_hyperparameters()\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr= self.init_lr)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr= self.max_lr, total_steps= self.total_steps)\n        return [optimizer], [scheduler]\n    def forward(self, images, formulas):\n        return self.model(images, formulas)\n\n    def training_step(self, batch, batch_idx):\n        images, formulas = batch\n        targets = formulas[:,1:]\n        formulas_in = formulas[:,:-1]\n        outputs = self.model(images, formulas_in)\n        bs, t, _ = outputs.size()\n        outputs = outputs.reshape(bs * t, -1)\n\n        \n        loss = self.criterion(outputs, targets.reshape(-1))\n        self.log(\"train loss\", loss, on_step=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        images, formulas = batch\n        targets = formulas[:,1:]\n        formulas_in = formulas[:,:-1]\n        outputs = self.model(images, formulas_in)\n        bs, t, _ = outputs.size()\n        outputs = outputs.reshape(bs * t, -1)\n\n        \n        loss = self.criterion(outputs, targets.reshape(-1))\n\n        predicts = [text.tokenize(text.int2text_n(self.model.decoder.generate_formulas(self.model.encoder(i.unsqueeze(0))\n                                                                                       , max_len=150, type = 'greedy'))) for i in images]\n\n        truths = [text.tokenize(text.int2text_n(i)) for i in formulas]\n\n        edit_dist = edit_distance(predicts, truths)\n        bleu =  calculate_bleu_score(predicts, truths)\n        em = exact_match_score(predicts, truths)\n        \n        self.log('val  loss', loss, on_step= True)\n        self.log('val edit_dist', edit_dist, on_step= True)\n        self.log('val bleu score', bleu, on_step= True)\n        self.log('exact math', em, on_step= True)\n\n    \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        images, formulas = batch\n        targets = formulas[:,1:]\n        formulas_in = formulas[:,:-1]\n        outputs = self.model(images, formulas_in)\n        bs, t, _ = outputs.size()\n        outputs = outputs.reshape(bs * t, -1)\n\n        \n        loss = self.criterion(outputs, targets.reshape(-1))\n        \n        predicts = [text.tokenize(text.int2text_n(self.model.decoder.generate_formulas(self.model.encoder(i.unsqueeze(0)),\n                                                                                       max_len=150, type = 'greedy'))) for i in images]\n\n        truths = [text.tokenize(text.int2text_n(i)) for i in formulas]\n\n        edit_dist = edit_distance(predicts, truths)\n        bleu =  calculate_bleu_score(predicts, truths)\n        em = exact_match_score(predicts, truths)\n        \n        self.log('test  loss', loss)\n        self.log('test edit_dist', edit_dist)\n        self.log('test bleu score', bleu)\n#         self.log('exact math', em)\n\n    \n        return loss\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:46:26.246016Z","iopub.execute_input":"2022-10-26T15:46:26.246417Z","iopub.status.idle":"2022-10-26T15:46:26.267236Z","shell.execute_reply.started":"2022-10-26T15:46:26.246359Z","shell.execute_reply":"2022-10-26T15:46:26.265836Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# 5adc273c49fc5b2641ffbfcce7c24338b108c7cb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:19:26.959993Z","iopub.execute_input":"2022-10-26T15:19:26.960368Z","iopub.status.idle":"2022-10-26T15:19:32.617360Z","shell.execute_reply.started":"2022-10-26T15:19:26.960336Z","shell.execute_reply":"2022-10-26T15:19:32.616207Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning import Trainer\n\nwandb_logger = WandbLogger(project=\"im2lax_paper\")","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:19:35.346587Z","iopub.execute_input":"2022-10-26T15:19:35.347010Z","iopub.status.idle":"2022-10-26T15:19:42.392705Z","shell.execute_reply.started":"2022-10-26T15:19:35.346976Z","shell.execute_reply":"2022-10-26T15:19:42.391697Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilongds145\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20221026_151935-292sbgfb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/philongds145/im2lax_paper/runs/292sbgfb\" target=\"_blank\">eager-sea-22</a></strong> to <a href=\"https://wandb.ai/philongds145/im2lax_paper\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}}]},{"cell_type":"code","source":"embed_size=80 \nvocab_size = n_class\nencoder_dim=512\ndecoder_dim=256\ninit_lr = 1e-4\nmax_lr = 1e-3\ntotal_steps = epochs * bs\nmodel = Image2LatexModel(init_lr= init_lr, max_lr= max_lr, total_steps= total_steps,\n                         enc_out_dim = encoder_dim, out_size = vocab_size,\n                         emb_size= embed_size, dec_rnn_h = decoder_dim , text = text)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:46:30.129907Z","iopub.execute_input":"2022-10-26T15:46:30.130268Z","iopub.status.idle":"2022-10-26T15:46:30.181002Z","shell.execute_reply.started":"2022-10-26T15:46:30.130237Z","shell.execute_reply":"2022-10-26T15:46:30.180132Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback  = pl.callbacks.ModelCheckpoint(dirpath='./', save_on_train_epoch_end=True)\nlr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\ntrainer = pl.Trainer(logger=wandb_logger, callbacks=[ checkpoint_callback, lr_monitor],\n                     max_epochs=epochs, accelerator='gpu')\ntrainer.test(datamodule=dm, model=model, ckpt_path='../input/ckpt-5/epoch5bn.ckpt')","metadata":{"execution":{"iopub.status.busy":"2022-10-26T15:46:33.237915Z","iopub.execute_input":"2022-10-26T15:46:33.238277Z","iopub.status.idle":"2022-10-26T15:55:35.975614Z","shell.execute_reply.started":"2022-10-26T15:46:33.238246Z","shell.execute_reply":"2022-10-26T15:55:35.974638Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"Testing: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662fc12ede2e4621b48a25c833332d20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m       test  loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6824802160263062    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m     test bleu score     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5059991478919983    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m     test edit_dist      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5914739966392517    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test  loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6824802160263062     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test bleu score      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5059991478919983     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test edit_dist       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5914739966392517     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"[{'test  loss': 0.6824802160263062,\n  'test edit_dist': 0.5914739966392517,\n  'test bleu score': 0.5059991478919983}]"},"metadata":{}}]}]}